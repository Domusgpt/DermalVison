# DermalVision — Professional Coding Agent Implementation Prompt

> **This document is a comprehensive directive for an autonomous coding agent to build the DermalVision application.**
> It must be used alongside `DERMALVISION_DEV_TRACK_AND_ARCHITECTURE.md` for complete architectural context.

---

## Preamble

You are a professional software engineering agent tasked with building **DermalVision**, a cross-platform mobile AI skin monitoring application. You will work autonomously through phased implementation steps, performing parallel work where dependencies allow and sequential analysis where required.

**Reference Documents (MUST READ FIRST)**:
- `DERMALVISION_DEV_TRACK_AND_ARCHITECTURE.md` — Complete system architecture, data models, feature taxonomy, AI pipeline design, UI specifications, and development roadmap
- This document — Implementation instructions, environment setup, coding standards, testing protocols, and agent workflow directives

---

## 1. Environment Setup & Prerequisites

### 1.1 Required SDK/Tool Installation

Execute the following setup steps before any implementation work. Verify each step before proceeding.

```bash
# === FLUTTER SDK ===
# Install Flutter SDK (latest stable channel)
# Verify with:
flutter --version          # Expected: Flutter 3.x.x
flutter doctor             # All checks should pass for target platforms
dart --version             # Expected: Dart 3.x.x

# === FIREBASE CLI ===
npm install -g firebase-tools
firebase --version         # Expected: 13.x+
firebase login             # Authenticate with project owner's Google account

# === GOOGLE CLOUD SDK ===
# Install gcloud CLI
gcloud --version
gcloud auth login
gcloud auth application-default login
gcloud config set project dermalvision-prod  # Replace with actual project ID

# === ANDROID SDK ===
# Android Studio installed with:
# - Android SDK 34+
# - Android SDK Build-Tools 34+
# - Android SDK Command-line Tools
# - Android Emulator
# Verify with:
flutter doctor --android-licenses

# === iOS (macOS only) ===
# Xcode 15+ installed
# CocoaPods installed:
sudo gem install cocoapods
pod --version

# === ADDITIONAL TOOLS ===
npm install -g @anthropic-ai/claude-code  # For agent integration testing
pip install google-cloud-aiplatform       # For Vertex AI model management
pip install mediapipe                     # For model testing/conversion
```

### 1.2 Firebase Project Configuration

```bash
# Create Firebase project (or use existing)
firebase projects:create dermalvision-prod --display-name "DermalVision"

# Enable required services:
firebase init firestore     # Initialize Firestore
firebase init storage       # Initialize Storage
firebase init functions     # Initialize Cloud Functions (Node.js 20)
firebase init hosting       # For web admin dashboard (optional)

# Enable Auth providers in Firebase Console:
# - Email/Password
# - Google Sign-In
# - Apple Sign-In
# - Anonymous Auth (for first-free-analysis flow)

# Enable Firebase services via gcloud:
gcloud services enable firestore.googleapis.com
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable run.googleapis.com
gcloud services enable aiplatform.googleapis.com
gcloud services enable vision.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable secretmanager.googleapis.com
```

### 1.3 Required Auth Tokens & API Keys

Create a `.env` file (NEVER commit this) and a Firebase config:

```bash
# === .env (local development only, add to .gitignore) ===

# Firebase (auto-generated by FlutterFire CLI)
# These come from: flutterfire configure

# Google Cloud
GCLOUD_PROJECT_ID=dermalvision-prod
GCLOUD_REGION=us-central1

# Vertex AI
VERTEX_AI_ENDPOINT_MEDGEMMA=projects/dermalvision-prod/locations/us-central1/endpoints/<endpoint_id>
VERTEX_AI_ENDPOINT_ACNE=projects/dermalvision-prod/locations/us-central1/endpoints/<endpoint_id>
VERTEX_AI_ENDPOINT_WRINKLE=projects/dermalvision-prod/locations/us-central1/endpoints/<endpoint_id>

# Gemini API (for SkinShurpa)
GEMINI_API_KEY=<your_gemini_api_key>
# OR use Vertex AI Gemini (preferred for production):
# Access via gcloud auth — no separate key needed

# RevenueCat
REVENUECAT_API_KEY_APPLE=<your_apple_key>
REVENUECAT_API_KEY_GOOGLE=<your_google_key>
REVENUECAT_WEBHOOK_SECRET=<webhook_secret>

# Weather API (for environmental correlation, Phase 7)
OPENWEATHER_API_KEY=<your_key>
```

```bash
# === Firebase Configuration ===
# Run FlutterFire CLI to auto-generate platform configs:
dart pub global activate flutterfire_cli
flutterfire configure --project=dermalvision-prod

# This generates:
# - lib/firebase_options.dart
# - android/app/google-services.json
# - ios/Runner/GoogleService-Info.plist
# - ios/firebase_app_id_file.json
```

### 1.4 Secret Management (Production)

```bash
# Store secrets in Google Secret Manager for Cloud Functions:
echo -n "YOUR_REVENUECAT_SECRET" | gcloud secrets create revenuecat-webhook-secret --data-file=-
echo -n "YOUR_GEMINI_KEY" | gcloud secrets create gemini-api-key --data-file=-

# Grant Cloud Functions access:
gcloud secrets add-iam-policy-binding revenuecat-webhook-secret \
  --member="serviceAccount:dermalvision-prod@appspot.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"
```

---

## 2. Flutter Project Initialization

### 2.1 Create Project

```bash
flutter create dermalvision \
  --org com.dermalvision \
  --description "AI Skin Monitoring & Analysis" \
  --platforms android,ios \
  --template app

cd dermalvision
```

### 2.2 Dependencies (pubspec.yaml)

```yaml
name: dermalvision
description: "DermalVision — The AI Skin Scope"
version: 0.1.0+1
publish_to: 'none'

environment:
  sdk: ^3.5.0
  flutter: ">=3.24.0"

dependencies:
  flutter:
    sdk: flutter

  # === STATE MANAGEMENT ===
  flutter_riverpod: ^2.6.1
  riverpod_annotation: ^2.6.1

  # === NAVIGATION ===
  go_router: ^14.6.2

  # === FIREBASE ===
  firebase_core: ^3.8.1
  firebase_auth: ^5.3.4
  firebase_cloud_firestore: # Use cloud_firestore
  cloud_firestore: ^5.6.0
  firebase_storage: ^12.3.7
  firebase_messaging: ^15.1.6
  firebase_analytics: ^11.3.6
  firebase_crashlytics: ^4.2.1
  firebase_remote_config: ^5.2.1

  # === CAMERA & IMAGE ===
  camera: ^0.11.0+2
  image_picker: ^1.1.2
  image_cropper: ^8.0.2
  image: ^4.3.0
  photo_view: ^0.15.0

  # === ML / AI ===
  tflite_flutter: ^0.11.0
  google_ml_kit: ^0.18.0
  google_mlkit_face_detection: ^0.12.0
  google_mlkit_face_mesh_detection: ^0.1.0

  # === NETWORKING ===
  dio: ^5.7.0
  retrofit: ^4.4.1

  # === LOCAL STORAGE ===
  hive: ^4.0.7
  hive_flutter: ^1.1.0
  shared_preferences: ^2.3.4
  flutter_secure_storage: ^9.2.4

  # === NOTIFICATIONS ===
  flutter_local_notifications: ^17.2.4
  workmanager: ^0.5.2

  # === SUBSCRIPTIONS ===
  purchases_flutter: ^8.3.0  # RevenueCat

  # === UI / ANIMATION ===
  flutter_animate: ^4.5.0
  fl_chart: ^0.69.2
  shimmer: ^3.0.0
  cached_network_image: ^3.4.1
  flutter_svg: ^2.0.16
  lottie: ^3.1.3

  # === SENSORS ===
  sensors_plus: ^6.1.1

  # === UTILITIES ===
  uuid: ^4.5.1
  intl: ^0.19.0
  freezed_annotation: ^2.4.6
  json_annotation: ^4.9.0
  equatable: ^2.0.7
  path_provider: ^2.1.5
  permission_handler: ^11.3.1
  url_launcher: ^6.3.1
  share_plus: ^10.1.3
  connectivity_plus: ^6.1.1
  package_info_plus: ^8.1.3

dev_dependencies:
  flutter_test:
    sdk: flutter
  flutter_lints: ^5.0.0
  build_runner: ^2.4.13
  riverpod_generator: ^2.6.3
  freezed: ^2.5.7
  json_serializable: ^6.8.0
  retrofit_generator: ^9.1.5
  mockito: ^5.4.4
  mocktail: ^1.0.4
  integration_test:
    sdk: flutter

flutter:
  uses-material-design: true

  shaders:
    - lib/shared/shaders/iridescent.frag
    - lib/shared/shaders/liquid_glass.frag
    - lib/shared/shaders/reactive_touch.frag
    - lib/shared/shaders/depth_blur.frag
    - lib/shared/shaders/heat_overlay.frag

  assets:
    - assets/images/
    - assets/icons/
    - assets/models/
    - assets/animations/
```

### 2.3 Android Configuration

```groovy
// android/app/build.gradle additions:
android {
    defaultConfig {
        minSdkVersion 24  // Required for CameraX + ML Kit
        targetSdkVersion 34
        multiDexEnabled true
    }

    buildTypes {
        release {
            signingConfig signingConfigs.release
            minifyEnabled true
            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'
        }
    }

    // TFLite model files should not be compressed
    aaptOptions {
        noCompress "tflite"
    }
}
```

### 2.4 iOS Configuration

```xml
<!-- ios/Runner/Info.plist additions: -->
<key>NSCameraUsageDescription</key>
<string>DermalVision needs camera access to capture skin monitoring photos</string>
<key>NSPhotoLibraryUsageDescription</key>
<string>DermalVision needs photo library access to import existing skin photos</string>
<key>NSMicrophoneUsageDescription</key>
<string>DermalVision uses the microphone for voice interaction with SkinShurpa</string>
<key>NSMotionUsageDescription</key>
<string>DermalVision uses motion sensors to guide consistent photo angles</string>
<key>UIBackgroundModes</key>
<array>
    <string>fetch</string>
    <string>remote-notification</string>
    <string>processing</string>
</array>
```

---

## 3. Agent Workflow Directives

### 3.1 Execution Model

You MUST follow this execution model:

```
PHASE EXECUTION ORDER:
═══════════════════════

For each phase in the roadmap (Phase 0 → Phase 8):

1. PLAN (Sequential)
   ├── Read the phase requirements from the architecture document
   ├── Identify all tasks and their dependency graph
   ├── Create a task list with clear dependency annotations
   └── Identify which tasks can be parallelized

2. IMPLEMENT (Parallel where possible)
   ├── Independent tasks: Execute in parallel
   │   Example: Auth UI + Theme setup + Router config (no dependencies)
   ├── Dependent tasks: Execute sequentially
   │   Example: Camera screen → AR overlay → Ghost image (each depends on previous)
   └── For each task:
       ├── Write implementation code
       ├── Write unit tests alongside (TDD when appropriate)
       ├── Run tests immediately after implementation
       └── Fix any failures before moving to next task

3. INTEGRATE (Sequential)
   ├── Wire parallel-implemented components together
   ├── Run integration tests
   ├── Fix integration issues
   └── Verify end-to-end flow for this phase

4. VALIDATE (Sequential)
   ├── Run full test suite (unit + widget + integration)
   ├── Run flutter analyze (zero warnings)
   ├── Run dart format (all files formatted)
   ├── Performance check (if UI-related phase)
   └── Security check (if data-related phase)

5. DOCUMENT (Parallel with validation where possible)
   ├── Update API documentation
   ├── Add inline documentation for complex logic
   ├── Update CHANGELOG.md
   └── Create phase completion report

6. COMMIT
   ├── Create descriptive commit message
   ├── Tag phase completion (e.g., v0.1.0-phase0)
   └── Push to development branch
```

### 3.2 Parallel Execution Rules

```
CAN run in parallel:
├── Independent feature modules (auth + theme + router in Phase 0)
├── Independent model classes (User model + Zone model + Session model)
├── Independent UI screens (if no shared state dependencies)
├── Unit tests for independent modules
├── Documentation for completed features
└── Shader files (each is independent)

MUST run sequentially:
├── Firebase setup → Firebase SDK integration → Auth implementation
├── Camera screen → AR overlay → Ghost image → Auto-capture
├── Model training → Model deployment → Model integration
├── Firestore schema → Repository layer → Provider layer → UI
├── Any task that depends on output of a previous task
└── Integration testing (after all unit-tested components exist)
```

### 3.3 Code Quality Gates

Before moving to the next phase, ALL of these must pass:

```bash
# 1. Static analysis — zero issues
flutter analyze --no-fatal-infos

# 2. Formatting — all files formatted
dart format --set-exit-if-changed .

# 3. Tests — all passing
flutter test --coverage

# 4. Coverage — minimum 80% for business logic
# (UI coverage targets lower — 60%)
genhtml coverage/lcov.info -o coverage/html
# Verify coverage meets thresholds

# 5. Build — both platforms build successfully
flutter build apk --debug
flutter build ios --debug --no-codesign  # macOS only
```

---

## 4. Implementation Details by Phase

### 4.1 Phase 0 — Foundation

**Parallel Track A: Project Structure & Theme**
```
Tasks (can run simultaneously):
├── Create folder structure (see architecture doc, Section 5.2)
├── Implement DermalVision color system (Section 11.1)
│   └── lib/app/theme/dermal_theme.dart
│   └── lib/app/theme/color_tokens.dart
├── Implement typography system (Section 11.2)
├── Create Material 3 ThemeData
└── Create placeholder screens for all major routes
```

**Parallel Track B: Firebase & Auth**
```
Tasks (can run simultaneously):
├── Run flutterfire configure
├── Initialize Firebase in main.dart
├── Implement Firebase Auth service
│   ├── Email/password sign up + sign in
│   ├── Google Sign-In
│   ├── Apple Sign-In
│   └── Anonymous auth (for try-before-signup)
├── Create auth state Riverpod provider
└── Build auth UI screens (login, signup, forgot password)
```

**Parallel Track C: Navigation & State**
```
Tasks (can run simultaneously):
├── Configure GoRouter with all route definitions
│   ├── Auth guard (redirect unauthenticated users)
│   ├── Onboarding guard (redirect incomplete onboarding)
│   └── Deep link handling setup
├── Set up Riverpod provider scope
├── Create base providers (auth, user profile, subscription)
└── Implement onboarding flow (skin type quiz, goals selection)
```

**Sequential after all tracks complete:**
```
├── Wire auth → router → home screen
├── Test full auth flow (signup → onboarding → home)
├── Run all quality gates
└── Commit Phase 0
```

### 4.2 Phase 1 — Camera System

This phase is PRIMARILY SEQUENTIAL due to heavy dependencies.

```
Sequential Pipeline:
1. Camera screen with live preview
   └── Use camera package, handle lifecycle, permissions
2. MediaPipe FaceMesh integration
   └── google_mlkit_face_mesh_detection for 468-point face mesh
   └── Calculate inter-pupillary distance → estimate camera distance
   └── Extract head pose (pitch, yaw, roll) from mesh
3. AR Guide Overlay
   └── CustomPainter overlay on camera preview
   └── Guide frame (green rectangle that shrinks when aligned)
   └── Distance indicator (bar with target marker)
   └── Angle indicator (3-axis rotation visualization)
   └── Readiness ring (fills based on all parameters matching)
4. Ghost Image Overlay
   └── Load previous session photo for this zone
   └── Render at 30% opacity over live camera feed
   └── User can toggle on/off
5. Lighting Assessment
   └── Read ambient light sensor (sensors_plus)
   └── Classify lighting quality (on-device ML or heuristic)
   └── Show lighting badge (red/yellow/green)
6. Body Zone System
   └── Zone data model with Firestore persistence
   └── Zone selection UI (body silhouette or list)
   └── Reference photo capture for new zones
7. Photo Capture Flow
   └── Auto-capture when readiness score > 95% for 1.5s
   └── Manual capture button always available
   └── Record all metadata (distance, angles, lighting, device)
   └── Save to local storage immediately
   └── Upload to Firebase Storage (background)
8. Image Processing Cloud Function
   └── Trigger on Storage upload
   └── Normalize white balance, exposure
   └── Generate thumbnails (128, 512, 1024)
   └── Update Firestore session document with URLs
```

### 4.3 Phase 2 — AI Analysis Pipeline

```
Parallel Track A: Model Preparation (can happen while Track B runs)
├── Set up Vertex AI project and endpoints
├── Deploy MedGemma 1.5 to Vertex AI endpoint
├── Train and deploy custom acne severity model
│   ├── Data: ISIC + SLICE-3D + custom labeled dataset
│   ├── Architecture: EfficientNet-B0 fine-tuned
│   ├── Output: 5-class severity (clear, mild, moderate, severe, very_severe)
│   └── Deploy as Vertex AI endpoint
├── Train and deploy wrinkle scoring model
│   ├── Regression model for depth/density estimation
│   └── Output: score 0-100
└── Prepare TFLite models for on-device screening

Parallel Track B: Backend Infrastructure
├── Analysis orchestrator Cloud Function
│   ├── Receives session ID, dispatches to appropriate models
│   ├── Parallel calls to: MedGemma, acne model, wrinkle model
│   ├── Waits for all results
│   └── Calls result aggregator
├── Image comparison engine (Cloud Run service)
│   ├── Takes two images + their metadata
│   ├── Performs image registration (align based on landmarks)
│   ├── Computes pixel-level deltas in L*a*b* color space
│   ├── Extracts change metrics (area, color, count deltas)
│   └── Returns structured comparison result
└── Result aggregator function
    ├── Combines all model outputs
    ├── Computes composite scores
    ├── Evaluates referral triggers
    ├── Generates natural language summary
    └── Stores to Firestore, triggers notification

Sequential: Client Integration (after both tracks)
├── Analysis request service (Riverpod provider)
├── Analysis result UI screens
│   ├── Result cards with scores and visualizations
│   ├── Heatmap overlay on photos
│   ├── Trend charts (fl_chart)
│   └── Comparison views (side-by-side, slider, overlay)
├── Referral trigger UI (concern banner → SkinShurpa)
└── Integration tests for full pipeline
```

### 4.4 Phase 3 — SkinShurpa

```
Sequential Pipeline (conversational AI requires careful testing):
1. Gemini API client service
   └── Vertex AI Gemini endpoint configuration
   └── Request/response models
   └── Streaming response support
2. System prompt construction
   └── See architecture doc Section 8.1 for base prompt
   └── Dynamic context injection (profile, history, concerns)
   └── Test with diverse scenarios
3. Function calling implementation
   └── Define all 6+ function schemas (Section 8.2)
   └── Implement function handlers
   └── Test function calling accuracy
4. Chat UI
   └── Message bubbles with markdown rendering
   └── Image reference cards (inline photo references)
   └── Action cards (structured suggestions)
   └── Typing indicator during streaming
   └── Function call result visualization
5. Session guide mode
   └── SkinShurpa narrates the photo capture process
   └── Reacts to camera state changes
   └── Provides real-time coaching
6. Results review mode
   └── Auto-invoked after analysis completes
   └── Explains findings conversationally
   └── Highlights changes, contextualizes
7. Conversation persistence
   └── Store conversations in Firestore
   └── Conversation history summarization for context window
```

### 4.5 Phase 4 — Premium UI (Depth Scroll & Shaders)

```
Parallel Track A: Depth Scroll System
├── DepthScrollView widget
│   ├── Custom ScrollController with scroll position tracking
│   ├── Per-item position calculation (normalizedPosition)
│   ├── Matrix4 transform computation (Section 9.2 of arch doc)
│   │   ├── Z-depth from position curve
│   │   ├── Sinusoidal x-offset (water-around-sphere)
│   │   ├── Scale reduction with depth
│   │   ├── Opacity fade with depth
│   │   └── Perspective projection (setEntry 3,2)
│   ├── Custom scroll physics (spring-based snapping)
│   └── Performance: limit visible items to 5, cull off-screen
├── OrbitalLayout widget
│   └── Card arrangement following spherical path
│   └── Touch/drag to rotate the orbital
└── ParallaxBackground widget
    └── Background layer that responds inversely to scroll
    └── Gradient shift with scroll position

Parallel Track B: Shader System
├── Shader infrastructure
│   ├── FragmentProgram loader utility
│   ├── Shader uniform manager (time, resolution, touch, scroll)
│   └── ShaderCard wrapper widget (renders child to texture, applies shader)
├── iridescent.frag — holographic shimmer
│   ├── Based on view angle (gyroscope input)
│   ├── Rainbow refraction at card edges
│   └── Intensity: 5-15% opacity, increases on interaction
├── liquid_glass.frag — viscous surface effect
│   ├── Ripple simulation on touch/scroll
│   ├── Surface tension physics
│   └── Dampening over time
├── reactive_touch.frag — touch response
│   ├── Radial wave from touch point
│   ├── Color shift at epicenter
│   └── Exponential decay
├── depth_blur.frag — camera DOF simulation
│   ├── Blur intensity from Z-depth uniform
│   ├── Gaussian kernel
│   └── Used for background cards
└── heat_overlay.frag — analysis heatmap
    ├── Color map: blue → green → yellow → red
    ├── Intensity from analysis data
    └── Semi-transparent overlay on photo

Sequential: Integration
├── Apply DepthScrollView to Home Dashboard
├── Apply ShaderCard to all card widgets
├── Wire gyroscope to iridescent shader
├── Wire scroll controller to liquid/depth shaders
├── Performance profiling (must maintain 60fps)
├── Implement battery-aware shader throttling
│   └── Disable shaders after 30s inactivity
│   └── Reduce shader complexity on low-battery
└── Device capability detection
    └── Disable shaders on OpenGL ES < 3.0
    └── Graceful fallback to standard Material cards
```

### 4.6 Phases 5–8

Continue following the same parallel/sequential pattern as defined in the architecture document's Phase descriptions (Section 19). Key implementation notes:

**Phase 5 (Subscription)**: RevenueCat SDK integration is mostly sequential. Feature gating system should use a `SubscriptionGate` widget wrapper.

**Phase 6 (Notifications)**: FCM setup and local notifications can be parallel tracks. Deep linking testing must be sequential after both are implemented.

**Phase 7 (Advanced Features)**: MCP server implementation, CLI tools, and advanced AI models can all be parallel tracks. Each is an independent module.

**Phase 8 (Launch)**: Entirely sequential — store submissions, legal review, load testing, and launch coordination.

---

## 5. Coding Standards

### 5.1 Dart/Flutter Standards

```dart
// === FILE NAMING ===
// snake_case for all files: camera_screen.dart, analysis_service.dart
// One public class per file (private helpers allowed)

// === CLASS NAMING ===
// PascalCase: CameraScreen, AnalysisResult, SkinShurpaProvider

// === STATE MANAGEMENT PATTERN ===
// Use Riverpod code generation (@riverpod annotation)

@riverpod
class ActiveSession extends _$ActiveSession {
  @override
  FutureOr<MonitoringSession?> build() async {
    return null; // No active session initially
  }

  Future<void> startSession(String zoneId) async {
    state = const AsyncLoading();
    state = await AsyncValue.guard(() async {
      // Create session logic
    });
  }
}

// === ERROR HANDLING PATTERN ===
// Use sealed classes for typed errors
sealed class AnalysisError {
  const AnalysisError();
}
class AnalysisNetworkError extends AnalysisError {
  final String message;
  const AnalysisNetworkError(this.message);
}
class AnalysisModelError extends AnalysisError {
  final String modelId;
  final String error;
  const AnalysisModelError(this.modelId, this.error);
}

// === DATA MODELS ===
// Use freezed for immutable data classes
@freezed
class AnalysisResult with _$AnalysisResult {
  const factory AnalysisResult({
    required String id,
    required String sessionId,
    required DateTime analyzedAt,
    required Map<String, ConditionResult> results,
    required CompositeScores compositeScores,
    required bool referralTriggered,
    String? referralReason,
    required String summary,
  }) = _AnalysisResult;

  factory AnalysisResult.fromJson(Map<String, dynamic> json) =>
      _$AnalysisResultFromJson(json);
}

// === WIDGET STRUCTURE ===
// Keep build methods under 80 lines
// Extract sub-widgets into separate classes, not methods
// Use const constructors wherever possible
class AnalysisResultCard extends StatelessWidget {
  const AnalysisResultCard({
    super.key,
    required this.result,
    required this.onTap,
  });

  final AnalysisResult result;
  final VoidCallback onTap;

  @override
  Widget build(BuildContext context) {
    // Implementation
  }
}
```

### 5.2 Cloud Functions Standards

```typescript
// === functions/src/index.ts ===
// Use 2nd gen Cloud Functions
import { onObjectFinalized } from "firebase-functions/v2/storage";
import { onDocumentCreated } from "firebase-functions/v2/firestore";
import { onRequest } from "firebase-functions/v2/https";

// === FUNCTION NAMING ===
// camelCase, descriptive: processUploadedImage, runAnalysisPipeline
export const processUploadedImage = onObjectFinalized(
  {
    bucket: "dermalvision-prod.appspot.com",
    region: "us-central1",
    memory: "1GiB",
    timeoutSeconds: 300,
  },
  async (event) => {
    // Process image: normalize, thumbnail, update Firestore
  }
);

// === ERROR HANDLING ===
// Always use try/catch with structured error logging
// Always set appropriate HTTP status codes for HTTPS functions
// Always validate input before processing
```

### 5.3 Shader Standards

```glsl
// === SHADER FILE HEADER ===
// Every .frag file must include:
#version 460 core

// DermalVision Shader: [name]
// Purpose: [description]
// Performance budget: [max instructions]
// Fallback: [what happens if shader unavailable]

#include <flutter/runtime_effect.glsl>

// Standard uniforms (all DermalVision shaders receive these)
uniform float uTime;           // Seconds since shader start
uniform vec2 uResolution;      // Viewport size in pixels
uniform vec2 uTouch;           // Touch position (normalized 0-1)
uniform float uScrollOffset;   // Current scroll position (normalized)
uniform float uDepth;          // Z-depth of this card (0=front, 1=far)
uniform float uInteraction;    // Interaction intensity (0=idle, 1=active)
uniform sampler2D uTexture;    // Card content rendered as texture

out vec4 fragColor;

void main() {
    // Implementation
    // MUST output to fragColor
    // MUST respect uDepth for depth-aware effects
    // MUST decay to no-effect when uInteraction approaches 0
}
```

---

## 6. Testing Protocols

### 6.1 Unit Test Pattern

```dart
// test/features/analysis/domain/analysis_scorer_test.dart
import 'package:flutter_test/flutter_test.dart';
import 'package:dermalvision/features/analysis/domain/analysis_scorer.dart';

void main() {
  group('AnalysisScorer', () {
    late AnalysisScorer scorer;

    setUp(() {
      scorer = AnalysisScorer();
    });

    group('acneSeverityScore', () {
      test('returns clear for zero count', () {
        final score = scorer.acneSeverityScore(count: 0, types: []);
        expect(score.grade, AcneGrade.clear);
        expect(score.numericScore, 0.0);
      });

      test('returns severe for high inflammatory count', () {
        final score = scorer.acneSeverityScore(
          count: 25,
          types: [AcneType.inflammatory, AcneType.cystic],
        );
        expect(score.grade, AcneGrade.severe);
        expect(score.numericScore, greaterThan(0.7));
      });

      test('triggers referral for cystic grade 4+', () {
        final score = scorer.acneSeverityScore(
          count: 30,
          types: [AcneType.cystic],
        );
        expect(score.referralTriggered, isTrue);
      });
    });

    group('moleABCDEScore', () {
      test('flags asymmetric moles', () {
        final score = scorer.moleABCDEScore(
          asymmetryIndex: 0.8,  // High asymmetry
          borderIrregularity: 0.2,
          colorVariance: 0.1,
          diameterMm: 4.0,
          evolutionDelta: 0.0,
        );
        expect(score.flags, contains(ABCDEFlag.asymmetry));
        expect(score.referralTriggered, isTrue);
      });
    });
  });
}
```

### 6.2 Widget Test Pattern

```dart
// test/shared/widgets/depth_scroll_view_test.dart
import 'package:flutter/material.dart';
import 'package:flutter_test/flutter_test.dart';
import 'package:dermalvision/shared/widgets/depth_scroll_view.dart';

void main() {
  group('DepthScrollView', () {
    testWidgets('renders all visible children', (tester) async {
      await tester.pumpWidget(
        MaterialApp(
          home: DepthScrollView(
            children: List.generate(10, (i) => Card(child: Text('Card $i'))),
          ),
        ),
      );

      // Center card should be fully visible
      expect(find.text('Card 0'), findsOneWidget);
    });

    testWidgets('applies depth transform on scroll', (tester) async {
      await tester.pumpWidget(
        MaterialApp(
          home: DepthScrollView(
            children: List.generate(10, (i) => Card(child: Text('Card $i'))),
          ),
        ),
      );

      // Scroll down
      await tester.drag(find.byType(DepthScrollView), const Offset(0, -200));
      await tester.pumpAndSettle();

      // Card 0 should now be transformed (moved to side, scaled down)
      final transform = tester.widget<Transform>(
        find.ancestor(of: find.text('Card 0'), matching: find.byType(Transform)),
      );
      expect(transform.transform, isNot(Matrix4.identity()));
    });
  });
}
```

### 6.3 Integration Test Pattern

```dart
// integration_test/monitoring_session_test.dart
import 'package:flutter_test/flutter_test.dart';
import 'package:integration_test/integration_test.dart';
import 'package:dermalvision/main.dart' as app;

void main() {
  IntegrationTestWidgetsFlutterBinding.ensureInitialized();

  testWidgets('full monitoring session flow', (tester) async {
    app.main();
    await tester.pumpAndSettle();

    // Navigate to camera
    await tester.tap(find.byKey(const Key('start_session_button')));
    await tester.pumpAndSettle();

    // Select zone
    await tester.tap(find.text('Left Cheek'));
    await tester.pumpAndSettle();

    // Verify camera guide is visible
    expect(find.byType(GuideOverlay), findsOneWidget);
    expect(find.byType(LightingIndicator), findsOneWidget);

    // Capture photo (simulate readiness)
    await tester.tap(find.byKey(const Key('capture_button')));
    await tester.pumpAndSettle();

    // Verify session saved
    expect(find.text('Photo captured'), findsOneWidget);

    // Verify analysis triggered
    expect(find.text('Analysis in progress'), findsOneWidget);
  });
}
```

### 6.4 ML Pipeline Test Pattern

```python
# tests/ml/test_analysis_pipeline.py
import pytest
from google.cloud import aiplatform

class TestAnalysisPipeline:
    """End-to-end tests for the ML analysis pipeline."""

    @pytest.fixture
    def vertex_endpoint(self):
        aiplatform.init(project="dermalvision-prod", location="us-central1")
        return aiplatform.Endpoint("projects/dermalvision-prod/locations/us-central1/endpoints/<id>")

    def test_medgemma_dermatology_inference(self, vertex_endpoint, sample_skin_image):
        """MedGemma returns valid dermatology classification."""
        response = vertex_endpoint.predict(instances=[{
            "image": sample_skin_image,
            "prompt": "Analyze this skin image for any notable conditions."
        }])
        assert response.predictions is not None
        assert len(response.predictions) > 0

    @pytest.mark.parametrize("skin_tone", [1, 2, 3, 4, 5, 6])
    def test_fairness_across_skin_tones(self, acne_model, skin_tone_test_set):
        """Model accuracy does not vary more than 5% across Fitzpatrick types."""
        results = {}
        for tone in range(1, 7):
            images = skin_tone_test_set[tone]
            accuracy = acne_model.evaluate(images)
            results[tone] = accuracy

        max_diff = max(results.values()) - min(results.values())
        assert max_diff < 0.05, f"Accuracy varies by {max_diff:.2%} across skin tones"
```

---

## 7. Documentation Protocols

### 7.1 Required Documentation Artifacts

```
docs/
├── API.md                    # Cloud Functions API documentation
├── DATA_MODEL.md             # Firestore schema with examples
├── ML_MODELS.md              # Model cards for each deployed model
│   ├── Performance metrics per Fitzpatrick type
│   ├── Training data description
│   ├── Known limitations
│   └── Version history
├── SHADER_GUIDE.md           # Shader system documentation
│   ├── Uniform reference
│   ├── Performance budgets
│   └── Fallback behavior
├── MCP_SERVER.md             # MCP tool/resource documentation
├── CHANGELOG.md              # Semantic versioning changelog
├── SECURITY.md               # Security practices and policies
├── CONTRIBUTING.md            # Contribution guidelines
└── ARCHITECTURE_DECISIONS.md  # ADRs (Architecture Decision Records)
```

### 7.2 Inline Documentation Standard

```dart
/// Computes the Z-axis depth transform matrix for a card in the DepthScrollView.
///
/// The transform creates the "water around a sphere" effect where cards:
/// - Start deep in the background (small, blurred, offset to side)
/// - Move to foreground center (full size, sharp, centered)
/// - Pass through to opposite side and recede
///
/// [normalizedPosition] ranges from -1.0 (far past) to 1.0 (far future),
/// with 0.0 being the focal center point.
///
/// [maxDepth] controls the maximum Z translation in logical pixels.
/// [maxHorizontalOffset] controls how far cards move to the sides.
///
/// Returns a [Matrix4] with perspective projection applied.
Matrix4 computeDepthTransform({
  required double normalizedPosition,
  double maxDepth = 300.0,
  double maxHorizontalOffset = 150.0,
}) {
  // Implementation
}
```

### 7.3 Commit Message Format

```
<type>(<scope>): <description>

[optional body]

[optional footer]

Types: feat, fix, refactor, test, docs, style, perf, ci, chore
Scopes: camera, analysis, shurpa, ui, auth, subscription, notifications, ml, infra

Examples:
feat(camera): implement ghost image overlay for position matching
fix(analysis): correct L*a*b* delta calculation for dark skin tones
perf(ui): reduce shader instruction count to maintain 60fps on mid-range devices
test(ml): add Fitzpatrick skin tone fairness tests for acne model
docs(mcp): document MCP server tools and resources
```

---

## 8. Security Checklist

Before each phase completion, verify:

```
□ No hardcoded API keys, secrets, or credentials in source code
□ .env file is in .gitignore
□ Firestore Security Rules tested (cannot access other users' data)
□ Storage Security Rules tested (cannot access other users' photos)
□ All user input sanitized before Firestore writes
□ SkinShurpa system prompt tested for prompt injection resistance
□ HTTPS enforced for all API calls
□ Certificate pinning enabled in release builds
□ Biometric lock option functions correctly
□ Photo metadata stripped of location data before upload (privacy)
□ SafeSearch validation active on all uploads
□ Error messages don't leak internal architecture details
□ Audit logging active for sensitive operations (data export, deletion)
```

---

## 9. Performance Budgets

```
Target Metrics:
├── App cold start:              < 2 seconds
├── Camera viewfinder FPS:       ≥ 60fps (with AR overlays active)
├── Depth scroll FPS:            ≥ 60fps (with 5 shader cards visible)
├── Photo capture to confirmation: < 500ms
├── Photo upload (10MB):         < 5 seconds on 4G
├── Analysis (cloud):            < 30 seconds end-to-end
├── On-device screening:         < 200ms
├── SkinShurpa first response:   < 2 seconds (streaming)
├── Screen transition:           < 300ms
├── App size (APK):              < 80MB (before asset download)
├── App size (IPA):              < 100MB
├── Memory usage (idle):         < 150MB
├── Memory usage (camera active): < 300MB
└── Battery drain (15min session): < 5% (with shaders active)
```

---

## 10. Deployment Checklist

### 10.1 Pre-Launch

```
□ All Phase 0–7 quality gates passed
□ Security audit completed by independent reviewer
□ Legal review of disclaimers, privacy policy, terms of service
□ Medical disclaimer reviewed by health law attorney
□ Accessibility audit (WCAG 2.1 AA minimum)
□ App Store screenshot and metadata prepared
□ Play Store listing and metadata prepared
□ Load testing: 1000 concurrent analysis requests
□ Disaster recovery tested (Firestore backup + restore)
□ RevenueCat production environment verified
□ FCM production certificates configured
□ Firebase Security Rules deployed to production
□ Cloud Functions deployed to production
□ Vertex AI endpoints scaled for expected load
□ Monitoring and alerting configured
□ On-call rotation established
□ Beta testing feedback addressed
```

### 10.2 Launch Day

```
1. Deploy all Cloud Functions to production
2. Verify Vertex AI endpoints are healthy
3. Submit to App Store and Play Store
4. Monitor Crashlytics for early crash signals
5. Monitor Cloud Monitoring for infrastructure issues
6. Monitor RevenueCat for payment flow issues
7. Stand by for first 24 hours
```

---

## 11. Agent Integration Points

### 11.1 MCP Server Launch

```bash
# The DermalVision MCP server exposes skin data to external AI agents
# Implementation: Cloud Run service with MCP protocol handler

# Local development:
cd functions/mcp-server
npm run dev  # Starts MCP server on localhost:3001

# Production:
gcloud run deploy dermalvision-mcp \
  --source . \
  --region us-central1 \
  --allow-unauthenticated=false \
  --service-account=mcp-server@dermalvision-prod.iam.gserviceaccount.com
```

### 11.2 CLI Tool Distribution

```bash
# The dermalvision-cli is distributed via pub.dev
cd packages/dermalvision_cli
dart pub publish

# Users install with:
dart pub global activate dermalvision_cli

# Agent interaction pattern:
# External agents can use the CLI via subprocess:
dermalvision status --json          # Returns JSON status
dermalvision analyze --image <path> --json  # Returns JSON analysis
dermalvision export --zone <id> --format csv --output -  # Pipe to stdout
```

---

*This prompt document provides complete implementation instructions for an autonomous coding agent. It must be used in conjunction with `DERMALVISION_DEV_TRACK_AND_ARCHITECTURE.md` for full architectural context including data models, feature taxonomy, AI pipeline design, UI specifications, and the complete development roadmap.*

*The agent should begin with Section 1 (Environment Setup), verify all prerequisites, then proceed through Phase 0 following the parallel/sequential execution model defined in Section 3.*
